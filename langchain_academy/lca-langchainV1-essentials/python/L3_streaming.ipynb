{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483ab18e-f419-46ad-9bbe-171ffd05f983",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "<img src=\"./assets/LC_streaming.png\" width=\"400\">\n",
    "\n",
    "Streaming reduces the latency between generating data and the user receiving it.\n",
    "There are two types frequently used with Agents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0f22c-9724-46ce-baf3-60a2de701fb3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76bab7-fa52-46f4-86bc-b157067e0168",
   "metadata": {},
   "source": [
    "Load and/or check for needed environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fcfd93-0004-4ff1-9b60-0b21baf68c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY=****3d99\n",
      "LANGSMITH_TRACING=true\n",
      "LANGSMITH_PROJECT=****ials\n",
      "LANGSMITH_ENDPOINT=****.com\n",
      "GOOGLE_API_KEY=****zZa4\n",
      "DEFAULT_MODEL=****lash\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print results\n",
    "doublecheck_env(\".env\")\n",
    "\n",
    "# Declaring default model\n",
    "default_model = \"google_genai:gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=default_model,\n",
    "    system_prompt=\"You are a full-stack comedian\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2",
   "metadata": {},
   "source": [
    "## No Steaming (invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the full-stack developer get kicked out of the restaurant?\n",
      "\n",
      "Because they kept trying to *inspect element* on the menu and complained the *waiter's API* was poorly documented!\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]})\n",
    "print(result[\"messages\"][1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9",
   "metadata": {},
   "source": [
    "## values\n",
    "You have seen this streaming mode in our examples so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me a Dad joke\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "# Stream = values\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a Dad joke\"}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada88835-3c66-4241-b3d9-4f3d38390c86",
   "metadata": {},
   "source": [
    "## messages\n",
    "Messages stream data token by token - the lowest latency possible. This is perfect for interactive applications like chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun peeks up with a sleepy yawn,\n",
      "And paints the sky at the break of dawn.\n",
      "With colors soft, of pink and gold,\n",
      "A brand new story about to unfold.\n",
      "\n",
      "Little birds begin to tweet,\n",
      "A happy song, oh so sweet!\n",
      "The flowers stretch, and gently sway,\n",
      "Waving hello to the brand new day.\n",
      "\n",
      "A tiny squirrel climbs up a tree,\n",
      "As busy as a bee, you see!\n",
      "The gentle breeze begins to play,\n",
      "Whispering secrets all the way.\n",
      "\n",
      "So let's all smile and laugh and cheer,\n",
      "And chase away each tiny fear.\n",
      "For every day, a gift so grand,\n",
      "With wonders waiting close at hand!"
     ]
    }
   ],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write me a family friendly poem.\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(f\"{token.content}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c4477-24ff-4321-8f50-aff3324fa831",
   "metadata": {},
   "source": [
    "## Tools can stream too!\n",
    "Streaming generally means delivering information to the user before the final result is ready. There are many cases where this is useful. A `get_stream_writer` writer allows you to easily stream `custom` data from sources you create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('values', {'messages': [HumanMessage(content='What is the weather in São Paulo?', additional_kwargs={}, response_metadata={}, id='417d7959-6c1a-4806-976a-2ab14c519033')]})\n",
      "('values', {'messages': [HumanMessage(content='What is the weather in São Paulo?', additional_kwargs={}, response_metadata={}, id='417d7959-6c1a-4806-976a-2ab14c519033'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"S\\\\u00e3o Paulo\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--135cc16d-90fd-4804-b200-abbeef94e18c-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'São Paulo'}, 'id': 'ba7305ad-0441-43d6-a851-abf25d1f0950', 'type': 'tool_call'}], usage_metadata={'input_tokens': 48, 'output_tokens': 78, 'total_tokens': 126, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 62}})]})\n",
      "('custom', 'Looking up data for city: São Paulo')\n",
      "('custom', 'Acquired data for city: São Paulo')\n",
      "('values', {'messages': [HumanMessage(content='What is the weather in São Paulo?', additional_kwargs={}, response_metadata={}, id='417d7959-6c1a-4806-976a-2ab14c519033'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"S\\\\u00e3o Paulo\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--135cc16d-90fd-4804-b200-abbeef94e18c-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'São Paulo'}, 'id': 'ba7305ad-0441-43d6-a851-abf25d1f0950', 'type': 'tool_call'}], usage_metadata={'input_tokens': 48, 'output_tokens': 78, 'total_tokens': 126, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 62}}), ToolMessage(content=\"It's always sunny in São Paulo!\", name='get_weather', id='8fdb3f5d-6190-453d-9383-23622323d417', tool_call_id='ba7305ad-0441-43d6-a851-abf25d1f0950')]})\n",
      "('values', {'messages': [HumanMessage(content='What is the weather in São Paulo?', additional_kwargs={}, response_metadata={}, id='417d7959-6c1a-4806-976a-2ab14c519033'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"S\\\\u00e3o Paulo\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--135cc16d-90fd-4804-b200-abbeef94e18c-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'São Paulo'}, 'id': 'ba7305ad-0441-43d6-a851-abf25d1f0950', 'type': 'tool_call'}], usage_metadata={'input_tokens': 48, 'output_tokens': 78, 'total_tokens': 126, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 62}}), ToolMessage(content=\"It's always sunny in São Paulo!\", name='get_weather', id='8fdb3f5d-6190-453d-9383-23622323d417', tool_call_id='ba7305ad-0441-43d6-a851-abf25d1f0950'), AIMessage(content=\"It's always sunny in São Paulo!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--52a5d147-ee35-4a50-a8bd-5cafdc8de30b-0', usage_metadata={'input_tokens': 87, 'output_tokens': 9, 'total_tokens': 96, 'input_token_details': {'cache_read': 0}})]})\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    # stream any arbitrary data\n",
    "    writer(f\"Looking up data for city: {city}\")\n",
    "    writer(f\"Acquired data for city: {city}\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=default_model,\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in São Paulo?\"}]},\n",
    "    stream_mode=[\"values\", \"custom\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('custom', 'Looking up data for city: São Paulo')\n",
      "('custom', 'Acquired data for city: São Paulo')\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in São Paulo?\"}]},\n",
    "    stream_mode=[\"custom\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845c067-761f-46a0-817c-2cc42066ce9a",
   "metadata": {},
   "source": [
    "## Try different modes on your own!\n",
    "Modify the stream mode and the select to produce different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92943e4f-6c17-4fa3-ad00-f86464ba66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the weather in Cajamar?\n",
      "\n",
      "Looking up data for city: Cajamar\n",
      "Acquired data for city: Cajamar\n",
      "It's always sunny in Cajamar!\n",
      "It's always sunny in Cajamar!\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in Cajamar?\"}]},\n",
    "    stream_mode=[\"values\", \"custom\"],\n",
    "):\n",
    "    if chunk[0] == \"custom\":\n",
    "        print(chunk[1])\n",
    "    if chunk[0] == \"values\":\n",
    "        print(chunk[1]['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
