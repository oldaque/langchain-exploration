{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec258d68",
   "metadata": {},
   "source": [
    "# Getting Started with LangSmith & LangChain Tracing\n",
    "\n",
    "### Prerequisites and Crucial Tips\n",
    "\n",
    "-   **Enable Tracing:** Set the environment variable before running your code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-langsmith-api-key>\"\n",
    "```\n",
    "*Note:* In my case I'll use a .env file to store these variables.\n",
    "\n",
    "-   **Choose Your Decorator:**\n",
    "-   Use `@traceable` for instrumenting standalone functions.\n",
    "\n",
    "-   Use `@chain` when building LangChain runnables (or when you want automatic callback integration).\n",
    "\n",
    "-   **Background Flushing:**\\\n",
    "    For short scripts, make sure to call langchain.callbacks.tracers.wait_for_all_tracers() at the end so that your asynchronous trace logs are sent before exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb12af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY=****3d99\n",
      "LANGSMITH_PROJECT=****dive\n",
      "LANGSMITH_OTEL_ENABLED=true\n",
      "LANGSMITH_TRACING=true\n",
      "LANGSMITH_ENDPOINT=****.com\n",
      "GOOGLE_API_KEY=****zZa4\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print results\n",
    "doublecheck_env(\".env\")  # check environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c4335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: Basic Function Tracing with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d5d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_sum result: 7\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(run_type=\"tool\")\n",
    "def compute_sum(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "result = compute_sum(3, 4)\n",
    "print(\"compute_sum result:\", result)  # Expected: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2238d",
   "metadata": {},
   "source": [
    "#### **Execution of the basic function tracing in LangSmith:**\n",
    "<img src=\"./assets/basic_function_tracing.png\" align=\"left\" width=\"1300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639d5fc",
   "metadata": {},
   "source": [
    "## Example 2: Nested Traceable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712ff0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_y result: 12\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"InnerJob\")\n",
    "def job_x(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "@traceable(name=\"OuterJob\", tags=[\"batch\"])\n",
    "def job_y(values: list) -> int:\n",
    "    total = 0\n",
    "    for v in values:\n",
    "        total += job_x(v)  # Nested trace: InnerJob becomes a child of OuterJob\n",
    "    return total\n",
    "\n",
    "result = job_y([1, 2, 3])\n",
    "print(\"job_y result:\", result)  # Expected: (1*2 + 2*2 + 3*2) = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c58203",
   "metadata": {},
   "source": [
    "*Crucial Info:*\\\n",
    "Nested calls automatically link together via context; no manual parent passing is needed.\n",
    "\n",
    "#### **Execution of the nested function in LangSmith:**\n",
    "<img src=\"./assets/nested_traceable_functions.png\" align=\"left\" width=\"1300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03c1f5",
   "metadata": {},
   "source": [
    "## Example 3: Converting a Function into a Runnable with @chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad90be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting result: Hello, Alice!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "@chain\n",
    "def greet(data: dict) -> str:\n",
    "    # data should contain the key \"person\"\n",
    "    person = data[\"person\"]\n",
    "    # Simulate a greeting message (could call an LLM here)\n",
    "    return f\"Hello, {person}!\"\n",
    "\n",
    "# Now, `greet` is a Runnable with .invoke() available.\n",
    "with tracing_v2_enabled(project_name=\"GreetingApp\"):\n",
    "    result = greet.invoke({\"person\": \"Alice\"})\n",
    "    print(\"Greeting result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b6289",
   "metadata": {},
   "source": [
    "*Key Point:* The `@chain` decorator wraps your function into a Runnable that integrates with LangChain's callback system for tracing.\n",
    "\n",
    "#### **Execution of Runnable in LangSmith:**\n",
    "<img src=\"./assets/runnable.png\" align=\"left\" width=\"1300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a1479",
   "metadata": {},
   "source": [
    "**Observation:** In langchain, the `create_agent()` function works with runnables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010273a",
   "metadata": {},
   "source": [
    "## Example 4: Combining LangChain and Custom Tracing\n",
    "\n",
    "> Langchain composes traces together using chaining, while langsmith composes traces together using scoping (i.e. outer functions calling inner functions).\\\n",
    "> This is how they can work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc77988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed answer: 'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate  # [!code ++]\n",
    "# from langchain import LLMChain, PromptTemplate  # [!code --]\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langsmith import traceable, Client\n",
    "import os\n",
    "\n",
    "# Getting project name from .env file\n",
    "project_name = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "def parse_answer(text) -> str:\n",
    "    # If the answer has a 'content' attribute (typical for ChatMessage objects), use it.\n",
    "    if hasattr(text, \"content\"):\n",
    "        text = text.content\n",
    "    return f\"Parsed answer: {text.strip()!r}\"\n",
    "\n",
    "@traceable(run_type=\"chain\", project_name=project_name)\n",
    "def perform_chain():\n",
    "    \"\"\"\n",
    "    this function is traceable which means every chain runnning inside it\n",
    "    will be composed together with it\n",
    "    \"\"\"\n",
    "\n",
    "    # Compose a chain using the | operator (v1 style ✅).\n",
    "    prompt = PromptTemplate.from_template(\"Tell me a joke about coding\")  # [!code ++]\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "    chain = prompt | llm  # ✅ LCEL (LangChain Expression Language) - v1 standard\n",
    "    \n",
    "    input_data = {}  # No inputs needed for this simple prompt.\n",
    "    answer = chain.invoke(input_data)\n",
    "    final_answer = parse_answer(answer)\n",
    "    return final_answer\n",
    "\n",
    "# Trace the overall process by using the tracing context manager.\n",
    "with tracing_v2_enabled(project_name=project_name):\n",
    "    answer = perform_chain()\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23990bc9",
   "metadata": {},
   "source": [
    "> The composition of chains using `prompt | llm` was employed, which represents the modern LangChain Expression Language (LCEL) pattern in v1. This operator is the recommended method and entirely replaces legacy approaches such as `LLMChain`.\n",
    "\n",
    "> By applying the `@traceable` decorator with `run_type='chain'` and `project_name` to the `perform_chain` function, it was ensured that everything within it is traced as sub-runs. This includes the `prompt | llm` chain and its invocation, as well as the internal call to `parse_answer`, which is automatically traced without the need for an explicit decorator. This validates that the decorator correctly captures all nested operations.\n",
    "\n",
    "> The use of the `tracing_v2_enabled` context manager with its `project_name` established a root trace in the LangSmith project. All internal calls, such as the execution of `perform_chain`, automatically inherit this context, ensuring a cohesive tracing hierarchy. This behavior confirms the proper setup and inheritance of the tracing context.\n",
    "\n",
    "> As a result, a well-defined trace hierarchy was obtained in LangSmith, where `perform_chain` acts as the main execution of type 'chain'. Within it, the `prompt | llm` chain is detailed with its `prompt` and `ChatGoogleGenerativeAI` components, and the `parse_answer` function appears as a sub-run. This validates that the tracing structure was configured clearly, technically, and precisely, ready for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07924442",
   "metadata": {},
   "source": [
    "#### **Execution of the combined LangChain and Custom Tracing in LangSmith:**\n",
    "<img src=\"./assets/combined_langchain_and_custom_tracing.png\" align=\"left\" width=\"1300\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
